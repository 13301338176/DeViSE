{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "########## Hyperparameter ##########\n",
    "BATCH_SIZE = 200\n",
    "EPOCH_BOUND = 1000\n",
    "EARLY_STOP_CHECK_EPOCH = 30\n",
    "TAKE_CROSS_VALIDATION = False\n",
    "CROSS_VALIDATION = 5\n",
    "########## Hyperparameter ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, w=0.1):\n",
    "    initial = tf.truncated_normal(shape, stddev=w) #Outputs random values from a truncated normal distribution.\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape, w=0.1):\n",
    "    initial = tf.constant(w, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def cnn_model(features, labels, mode):\n",
    "    # Input Layer\n",
    "    # input layer shape should be [batch_size, image_width, image_height, channels] for conv2d\n",
    "    # set batch_size = -1 means batch_size = the number of input\n",
    "    print('input data shape: ', features)\n",
    "    input_layer = tf.reshape(features, [-1, 32, 32, 3])\n",
    "    print('input layer shape: ', input_layer.shape)\n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = weight_variable(shape=[5, 5, 3, 64]) #shape=[filter_height * filter_width * in_channels, output_channels]\n",
    "        conv = tf.nn.conv2d(input_layer, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = bias_variable(shape=[64], w=0.0)\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "       \n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "    print('pool1 shape: ', pool1)\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = weight_variable(shape=[5, 5, 64, 64])\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = bias_variable(shape=[64], w=0.1)\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "    print('pool2 shape: ', pool2)\n",
    "    \n",
    "    pool2_flat = tf.reshape(pool2, [-1, 8*8*64])\n",
    "    dense1 = tf.layers.dense(\n",
    "        inputs=pool2_flat,\n",
    "        units=1024, # number of neurons in the dense layer\n",
    "        activation=tf.nn.relu,\n",
    "        name='dense1')\n",
    "    dropout1 = tf.layers.dropout(\n",
    "        inputs=dense1,\n",
    "        rate=0.1,\n",
    "        training= mode=='TRAIN',\n",
    "        name='dropout1')\n",
    "    dense2 = tf.layers.dense(\n",
    "        inputs=dropout1,\n",
    "        units=1024, # number of neurons in the dense layer\n",
    "        activation=tf.nn.relu,\n",
    "        name='dense2')\n",
    "    dropout2 = tf.layers.dropout(\n",
    "        inputs=dense2,\n",
    "        rate=0.1,\n",
    "        training= mode=='TRAIN',\n",
    "        name='dropout2')\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout2, units=100, name='logits')\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def train(X_train, y_train, X_validate, y_validate, optimizer, epoch_bound, stop_threshold, batch_size, testing=False):\n",
    "\n",
    "    global saver\n",
    "    global predictions\n",
    "    \n",
    "    early_stop = 0\n",
    "    winner_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(optimizer, feed_dict={x: X_train[batch*batch_size:], \n",
    "                                               y: y_train[batch*batch_size:], \n",
    "                                               mode:'TRAIN'})\n",
    "            else:\n",
    "                sess.run(optimizer, feed_dict={x: X_train[batch*batch_size : (batch+1)*batch_size], \n",
    "                                               y: y_train[batch*batch_size : (batch+1)*batch_size], \n",
    "                                               mode:'TRAIN'})\n",
    "        \n",
    "        # split validation set into multiple mini-batches and start validating\n",
    "        cur_accuracy = 0.0\n",
    "        total_batches = int(X_validate.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                results = sess.run(predictions, feed_dict={x:X_validate[batch*batch_size:]\n",
    "                                                           , y:y_validate[batch*batch_size:]\n",
    "                                                           , mode:'EVAL'})\n",
    "                cur_accuracy += sess.run(tf.reduce_mean(tf.cast(tf.equal(y_validate[batch*batch_size:], results['classes'])\n",
    "                                                                , tf.float32)))\n",
    "            else:\n",
    "                results = sess.run(predictions, feed_dict={x:X_validate[batch*batch_size : (batch+1)*batch_size]\n",
    "                                                           , y:y_validate[batch*batch_size : (batch+1)*batch_size]\n",
    "                                                           , mode:'EVAL'})\n",
    "                cur_accuracy += sess.run(tf.reduce_mean(tf.cast(tf.equal(y_validate[batch*batch_size : (batch+1)*batch_size]\n",
    "                                                                         , results['classes']), tf.float32)))\n",
    "        cur_accuracy /= total_batches\n",
    "        print('Epoch: ', epoch, 'Accuracy: ', cur_accuracy)\n",
    "        \n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if winner_accuracy < cur_accuracy:\n",
    "            early_stop = 0\n",
    "            winner_accuracy = cur_accuracy\n",
    "            # save best model in testing phase\n",
    "            if testing == True:\n",
    "                save_path = saver.save(sess, \"./saved_model/cifar-100.ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        if early_stop == stop_threshold:\n",
    "            break\n",
    "\n",
    "    return winner_accuracy\n",
    "\n",
    "def unpickle(file):\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "            return dict # return dic keys: [b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data']\n",
    "\n",
    "# split dataset into training set and one validation set\n",
    "def split_folds(indices, Inputs, Labels, cross_validation, fold):\n",
    "    n = Inputs.shape[0]\n",
    "    if fold == cross_validation:\n",
    "        validation_size = n - (int(n/cross_validation) * (cross_validation-1))\n",
    "        X_train_idx, X_validate_idx = indices[:(n-validation_size)], indices[(n-validation_size):]\n",
    "        y_train_idx, y_validate_idx = indices[:(n-validation_size)], indices[(n-validation_size):]\n",
    "    else:\n",
    "        validation_size = int(n/cross_validation)\n",
    "        X_train_idx, X_validate_idx = np.concatenate((indices[:validation_size*(fold-1)], indices[validation_size*fold:]), axis=0), indices[(validation_size*(fold-1)):(validation_size*fold)]\n",
    "        y_train_idx, y_validate_idx = np.concatenate((indices[:validation_size*(fold-1)], indices[validation_size*fold:]), axis=0), indices[(validation_size*(fold-1)):(validation_size*fold)]\n",
    "    X_train, X_validate = np.array(Inputs[X_train_idx,:]), np.array(Inputs[X_validate_idx,:])\n",
    "    y_train, y_validate = np.array(Labels[y_train_idx]), np.array(Labels[y_validate_idx])\n",
    "    return X_train, y_train, X_validate, y_validate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape:  (50000, 3072)\n",
      "Train Label shape:  (50000,)\n"
     ]
    }
   ],
   "source": [
    "########## Data ##########\n",
    "# Data format:\n",
    "# data -- a 10000x3072 numpy array of uint8s. \n",
    "#         Each row of the array stores a 32x32 colour image. \n",
    "#         The first 1024 entries contain the red channel values, \n",
    "#         the next 1024 the green, and the final 1024 the blue. \n",
    "#         The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "# labels -- a list of 10000 numbers in the range 0-99. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "# Load training data\n",
    "train_set = unpickle('./Data/cifar-100/train')\n",
    "train_data = np.asarray(train_set[b'data'], dtype=np.float32) # shape (50000, 3072) 50000 images of 32x32x3 values\n",
    "train_labels = np.asarray(train_set[b'fine_labels'], dtype=np.int32)\n",
    "\n",
    "# Load testing data\n",
    "test_set = unpickle('./Data/cifar-100/test')\n",
    "eval_data =np.asarray(test_set[b'data'], dtype=np.float32) # shape (10000, 3072) 50000 images of 32x32x3 values\n",
    "eval_labels = np.asarray(test_set[b'fine_labels'], dtype=np.int32)\n",
    "\n",
    "print('Train Data shape: ',train_data.shape)\n",
    "print('Train Label shape: ', train_labels.shape)\n",
    "\n",
    "########## Data ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data shape:  Tensor(\"Placeholder:0\", shape=(?, 3072), dtype=float32)\n",
      "input layer shape:  (?, 32, 32, 3)\n",
      "pool1 shape:  Tensor(\"pool1:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "pool2 shape:  Tensor(\"pool2:0\", shape=(?, 8, 8, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "########## CNN classifier ##########\n",
    "x = tf.placeholder(tf.float32, [None, train_data.shape[1]])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "mode = tf.placeholder(tf.string, name='mode')\n",
    "\n",
    "logits = cnn_model(x, y, mode)\n",
    "\n",
    "# Calculate Loss (for both TRAIN and EVAL modes)\n",
    "onehot_labels = tf.one_hot(indices=tf.cast(y, tf.int32), depth=100)\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "# Training iteration (for TRAIN)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "# For prediction (for EVAL)\n",
    "probabilities = tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "predictions = {\n",
    "  # Generate predictions (for PREDICT and EVAL mode)\n",
    "  \"classes\": tf.argmax(input=probabilities, axis=1),\n",
    "  # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "  # `logging_hook`.\n",
    "  \"probabilities\": probabilities\n",
    "}\n",
    "\n",
    "########## CNN classifier ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Start training ##########\n",
      "restore the precious best model\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/cifar-100.ckpt\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.439999967813\n",
      "Epoch:  0 Accuracy:  0.439999967813\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.449999988079\n",
      "Epoch:  1 Accuracy:  0.449999988079\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.444999992847\n",
      "Epoch:  2 Accuracy:  0.444999992847\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.42999997735\n",
      "Epoch:  3 Accuracy:  0.42999997735\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.414999961853\n",
      "Epoch:  4 Accuracy:  0.414999961853\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.429999947548\n",
      "Epoch:  5 Accuracy:  0.429999947548\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.444999963045\n",
      "Epoch:  6 Accuracy:  0.444999963045\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.420000016689\n",
      "Epoch:  7 Accuracy:  0.420000016689\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.454999983311\n",
      "Epoch:  8 Accuracy:  0.454999983311\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.419999986887\n",
      "Epoch:  9 Accuracy:  0.419999986887\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.435000002384\n",
      "Epoch:  10 Accuracy:  0.435000002384\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.430000007153\n",
      "Epoch:  11 Accuracy:  0.430000007153\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.435000002384\n",
      "Epoch:  12 Accuracy:  0.435000002384\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.435000002384\n",
      "Epoch:  13 Accuracy:  0.435000002384\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.449999988079\n",
      "Epoch:  14 Accuracy:  0.449999988079\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.42999997735\n",
      "Epoch:  15 Accuracy:  0.42999997735\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.409999966621\n",
      "Epoch:  16 Accuracy:  0.409999966621\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.429999947548\n",
      "Epoch:  17 Accuracy:  0.429999947548\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.419999957085\n",
      "Epoch:  18 Accuracy:  0.419999957085\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.409999966621\n",
      "Epoch:  19 Accuracy:  0.409999966621\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.424999982119\n",
      "Epoch:  20 Accuracy:  0.424999982119\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.420000016689\n",
      "Epoch:  21 Accuracy:  0.420000016689\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.444999992847\n",
      "Epoch:  22 Accuracy:  0.444999992847\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.40000000596\n",
      "Epoch:  23 Accuracy:  0.40000000596\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.414999961853\n",
      "Epoch:  24 Accuracy:  0.414999961853\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.434999972582\n",
      "Epoch:  25 Accuracy:  0.434999972582\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.424999952316\n",
      "Epoch:  26 Accuracy:  0.424999952316\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.399999976158\n",
      "Epoch:  27 Accuracy:  0.399999976158\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.40000000596\n",
      "Epoch:  28 Accuracy:  0.40000000596\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.410000026226\n",
      "Epoch:  29 Accuracy:  0.410000026226\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.40499997139\n",
      "Epoch:  30 Accuracy:  0.40499997139\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.409999966621\n",
      "Epoch:  31 Accuracy:  0.409999966621\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.419999957085\n",
      "Epoch:  32 Accuracy:  0.419999957085\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.435000002384\n",
      "Epoch:  33 Accuracy:  0.435000002384\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.424999952316\n",
      "Epoch:  34 Accuracy:  0.424999952316\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.394999980927\n",
      "Epoch:  35 Accuracy:  0.394999980927\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.439999938011\n",
      "Epoch:  36 Accuracy:  0.439999938011\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.434999972582\n",
      "Epoch:  37 Accuracy:  0.434999972582\n",
      "X_validate shape: 200 total batch: 1\n",
      "cur acc: 0.394999951124\n",
      "Epoch:  38 Accuracy:  0.394999951124\n",
      "training with all the inputs, accuracy: 0.454999983311\n"
     ]
    }
   ],
   "source": [
    "########## Train ##########\n",
    "print(\"########## Start training ##########\")\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "# init saver to save model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# randomize dataset\n",
    "indices = np.random.permutation(train_data.shape[0])\n",
    "\n",
    "# start cross validation\n",
    "avg_accuracy = 0.0\n",
    "\n",
    "if TAKE_CROSS_VALIDATION == True:\n",
    "    for fold in range(1, CROSS_VALIDATION+1):\n",
    "        print(\"########## Fold:\", fold, \"##########\")\n",
    "        # init weights\n",
    "        sess.run(init)\n",
    "        # restore the precious best model\n",
    "        saver.restore(sess, \"./saved_model/cifar-100.ckpt\")\n",
    "        # split inputs into training set and validation set for each fold\n",
    "        X_train, y_train, X_validate, y_validate = split_folds(indices, train_data, train_labels, CROSS_VALIDATION, fold)\n",
    "        print('validate data: ', X_validate.shape)\n",
    "        print('validate label: ', y_validate.shape)\n",
    "        print('train data: ', X_train.shape)\n",
    "        print('train label: ', y_train.shape)\n",
    "\n",
    "        winner_accuracy = train(X_train, y_train, X_validate, y_validate\n",
    "                                , train_op, EPOCH_BOUND, EARLY_STOP_CHECK_EPOCH, BATCH_SIZE, testing=False)\n",
    "        avg_accuracy += winner_accuracy\n",
    "        print(\"Accuracy:\", winner_accuracy)\n",
    "    avg_accuracy /= cross_validation\n",
    "    print(\"Average accuracy of cross validation:\", avg_accuracy)\n",
    "\n",
    "# # training without cross_validation\n",
    "# indices_training = np.random.permutation(train_data.shape[0])\n",
    "# validate_indices = indices_training[:BATCH_SIZE]\n",
    "# train_indices = indices_training[BATCH_SIZE:]\n",
    "# validate_data, validate_labels = train_data[validate_indices], train_labels[validate_indices]\n",
    "# train_data, train_labels = train_data[train_indices], train_labels[train_indices]\n",
    "\n",
    "# # init weights\n",
    "# sess.run(init)\n",
    "# restore the precious best model\n",
    "if os.path.exists(\"./saved_model/cifar-100.ckpt.meta\") == True:\n",
    "    print(\"restore the precious best model\")\n",
    "    saver.restore(sess, \"./saved_model/cifar-100.ckpt\")\n",
    "else:\n",
    "    # init weights\n",
    "    sess.run(init)\n",
    "# randomize dataset\n",
    "indices = np.random.permutation(train_data.shape[0])\n",
    "Inputs, Labels = np.array(train_data[indices,:]), np.array(train_labels[indices])\n",
    "\n",
    "# get validation set with the size of a batch for early-stop\n",
    "X_train, y_train = Inputs[BATCH_SIZE:], Labels[BATCH_SIZE:]\n",
    "X_validate, y_validate = Inputs[:BATCH_SIZE], Labels[:BATCH_SIZE]\n",
    "\n",
    "# start training with all the inputs\n",
    "winner_accuracy = train(X_train, y_train, X_validate, y_validate\n",
    "                        , train_op, EPOCH_BOUND, EARLY_STOP_CHECK_EPOCH, BATCH_SIZE, testing=True)\n",
    "print(\"training with all the inputs, accuracy:\", winner_accuracy)\n",
    "\n",
    "\n",
    "sess.close()\n",
    "########## Train ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Start evaluating ##########\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/cifar-100.ckpt\n",
      "Test accuracy:  0.282700002193\n"
     ]
    }
   ],
   "source": [
    "########## Evaluate ##########\n",
    "# Evaluate the model and print results\n",
    "print(\"########## Start evaluating ##########\")\n",
    "sess = tf.Session()\n",
    "# restore the precious best model\n",
    "saver.restore(sess, \"./saved_model/cifar-100.ckpt\")\n",
    "\n",
    "testing_accuracy = 0.0\n",
    "for i in range(5):\n",
    "    results = sess.run(predictions, feed_dict={x:eval_data[i*2000:(i+1)*2000], y:eval_labels[i*2000:(i+1)*2000], mode:'EVAL'})\n",
    "    testing_accuracy += sess.run(tf.reduce_mean(tf.cast(tf.equal(eval_labels[i*2000:(i+1)*2000], results['classes']),\n",
    "                                                    tf.float32)))\n",
    "print('Test accuracy: ', testing_accuracy/5)\n",
    "sess.close()\n",
    "########## Evaluate ##########\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
